{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed428a5f-2b50-49bd-ba24-2773fc756f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import delta\n",
    "import mimesis\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc55a728-5149-4be1-afe7-8895491369cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark() -> SparkSession:\n",
    "    builder = (\n",
    "        pyspark.sql.SparkSession.builder.appName(\"TestApp\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "    )\n",
    "    spark = delta.configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7fe032-0a77-4385-97af-df120088af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(i: int = 100) -> list[dict]:\n",
    "    fake = mimesis.Generic()\n",
    "    output = [\n",
    "        {\n",
    "            \"name\": fake.person.name(),\n",
    "            \"surname\": fake.person.surname(),\n",
    "            \"birthday\": fake.datetime.date(1960, 2010),\n",
    "            \"email\": fake.person.email(),\n",
    "            \"country\": fake.address.country(),\n",
    "            \"state\": fake.address.state(),\n",
    "            \"city\": fake.address.city(),\n",
    "        }\n",
    "        for _ in range(i)\n",
    "    ]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990823b9-5ce8-4cc5-9431-de529240ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dir = os.getcwd()\n",
    "    spark = get_spark()\n",
    "\n",
    "    df = spark.createDataFrame(create_dataset(i=1_000_000))\n",
    "\n",
    "    df = df.select(\n",
    "        df.name, df.surname, df.birthday, df.email, df.country, df.state, df.city\n",
    "    )\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(dir + os.sep + \"people\")\n",
    "    df.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bede25b7-c95b-4659-a348-e66a46ccd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/17 15:05:52 WARN Utils: Your hostname, MacBookJulio.local resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface en0)\n",
      "24/02/17 15:05:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/dal/anaconda3/envs/delta_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dal/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dal/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2eec8c81-1b94-46dc-b788-3908b4ced951;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 236ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2eec8c81-1b94-46dc-b788-3908b4ced951\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/17 15:05:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------------------+-----------------+------------+----------------+\n",
      "|    name| surname|  birthday|               email|          country|       state|            city|\n",
      "+--------+--------+----------+--------------------+-----------------+------------+----------------+\n",
      "| Matthew|    Reed|2005-10-21| above1981@gmail.com|           Malawi|     Alabama|         Norwich|\n",
      "|  Golden|  Jordan|2008-02-08|elements1935@gmai...|     Sierra Leone|      Hawaii|          Edmond|\n",
      "| Dinorah|   White|1991-12-12| andy1850@yandex.com|          Eritrea|    Missouri|           Sandy|\n",
      "|   Myung|   Blair|1970-02-22|donate1938@outloo...|    United States|     Alabama|    West Chester|\n",
      "|   Thanh| Wiggins|1976-11-27|advance1823@yande...|            Chile|  New Jersey|     Perth Amboy|\n",
      "| Lashell| Johnson|2007-07-04|supervision1951@o...|           Guyana|    Oklahoma|         Roselle|\n",
      "| Pandora|Sandoval|1990-07-23| net1956@outlook.com|      Timor-Leste|North Dakota|       Montclair|\n",
      "| Tammera|   Cooke|2001-03-03|   pig1838@gmail.com|             Laos|     Arizona|     Porterville|\n",
      "|  Aubrey|    Pena|2003-06-15|relation2074@exam...| French Polynesia|   Tennessee|  East Cleveland|\n",
      "|Florinda|   Haley|1969-04-28|villages1818@live...|         Zimbabwe|South Dakota|North Lauderdale|\n",
      "|   Scott| Dejesus|2006-07-08|trainer1803@outlo...|       St. Martin|  Washington|         Clinton|\n",
      "|  Luanna|  Baxter|2007-01-30|vary1915@outlook.com|            Tonga|    Missouri| Upper Arlington|\n",
      "|Kathline|  Barber|1991-04-12|mounting1912@yand...|         Guernsey|       Texas|        Bay City|\n",
      "| Rolland|   Velez|1965-07-11| dealer1931@duck.com|    Liechtenstein|     Arizona|   Downers Grove|\n",
      "|Stephnie|  Murray|1975-07-22|uniform2003@proto...|Equatorial Guinea|      Hawaii|    Indian Trail|\n",
      "| Sherika|  Melton|1971-12-05|davidson1901@yaho...|           Israel|     Alabama|           Salem|\n",
      "|     Ned| Ratliff|1999-10-12|penetration1938@y...|       Seychelles|     Georgia|   Copperas Cove|\n",
      "| Shaunta|  Graham|1981-06-07|solaris1933@yahoo...|       Montserrat|    Oklahoma|         Burbank|\n",
      "|   Lucio|Gonzalez|2010-02-23|velocity1888@gmai...|             Mali|     Arizona|  Eagle Mountain|\n",
      "|Vincenzo|   Drake|1961-03-16|kingston2085@live...|         Thailand|     Indiana|        Martinez|\n",
      "+--------+--------+----------+--------------------+-----------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd6208-fa34-4804-ae3d-0094b6b9329c",
   "metadata": {},
   "source": [
    "#### setup env no OSX\n",
    "https://www.linkedin.com/pulse/set-up-pyspark-delta-lake-your-local-machine-marius-krol/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6829f-d63b-4c06-855f-3875ae7e038e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
